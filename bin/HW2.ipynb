{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1nBu7sZoJI0U6LzzywHqalc_YYTyJR_OC","authorship_tag":"ABX9TyPTb406XBRYDvF4ETkPN5Sd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"cb5c86c1d9ff40b08b8cd4524d7f3385":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_263c4cf549b34cb18bd901edf03b8703","IPY_MODEL_e73396ccf79b47eea7dc2b80a4c25825","IPY_MODEL_b01e466858a74d07b431888d9dad46a0"],"layout":"IPY_MODEL_1933e7d798f74194bcca25dc722d45d5"}},"263c4cf549b34cb18bd901edf03b8703":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04a5aef3d83d475ea24f1040cffa89da","placeholder":"​","style":"IPY_MODEL_85ea790f52c7452b8f7f45629aa2e502","value":"Downloading builder script: 100%"}},"e73396ccf79b47eea7dc2b80a4c25825":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b59f29ea77a4b16806e9e3b3e59541e","max":6270,"min":0,"orientation":"horizontal","style":"IPY_MODEL_22f627ab23254ed19a6cd98f4d51a79e","value":6270}},"b01e466858a74d07b431888d9dad46a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f35ec3785214171a315fccbfcd78826","placeholder":"​","style":"IPY_MODEL_e44364ebcd7f44a9a1c1e4792e1cc33d","value":" 6.27k/6.27k [00:00&lt;00:00, 446kB/s]"}},"1933e7d798f74194bcca25dc722d45d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04a5aef3d83d475ea24f1040cffa89da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85ea790f52c7452b8f7f45629aa2e502":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b59f29ea77a4b16806e9e3b3e59541e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22f627ab23254ed19a6cd98f4d51a79e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f35ec3785214171a315fccbfcd78826":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e44364ebcd7f44a9a1c1e4792e1cc33d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# Ensure this is a CODE cell in Colab\n","\n","# @title 1. Setup: Install Libraries\n","# Install required libraries\n","!pip install google-generativeai python-dotenv jiwer evaluate rouge_score groq --quiet\n","print(\"Libraries installed successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tZN80ojYPgfm","executionInfo":{"status":"ok","timestamp":1749649597636,"user_tz":-120,"elapsed":17024,"user":{"displayName":"Filippo CROCE","userId":"06292711882020938839"}},"outputId":"e6fc0def-436a-45d9-80f2-67c018c08972"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.6/129.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Libraries installed successfully.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","DRIVE_PATH = \"/content/drive/MyDrive/Prompto_Ergo_Sum_HW2/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHAyWYLEb2TH","executionInfo":{"status":"ok","timestamp":1749649620186,"user_tz":-120,"elapsed":14450,"user":{"displayName":"Filippo CROCE","userId":"06292711882020938839"}},"outputId":"d710e85a-1269-42ab-bbb9-7d8ad5e95894"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import json\n","import os\n","import re\n","import time\n","from google import genai\n","from dotenv import load_dotenv\n","import jiwer\n","import evaluate\n","import difflib\n","from groq import Groq\n","\n","try:\n","    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n","    if not GOOGLE_API_KEY:\n","        raise ValueError(\"API Key not found in Colab Secrets. Please add it or use Option 2.\")\n","    print(\"GOOGLE_API_KEY loaded from Colab Secrets.\")\n","except Exception as e:\n","    print(f\"Could not load API key from Colab Secrets: {e}\")\n","    print(\"Please use Option 2 below if you haven't set up the secret, OR ensure the secret name is 'GOOGLE_API_KEY'.\")\n","    GOOGLE_API_KEY = None # Ensure it's None if not found\n","\n","try:\n","    rouge_metric_evaluator = evaluate.load(\"rouge\")\n","    print(\"ROUGE metric evaluator loaded.\")\n","except Exception as e:\n","    print(f\"Error loading ROUGE metric: {e}\")\n","    rouge_metric_evaluator = None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211,"referenced_widgets":["cb5c86c1d9ff40b08b8cd4524d7f3385","263c4cf549b34cb18bd901edf03b8703","e73396ccf79b47eea7dc2b80a4c25825","b01e466858a74d07b431888d9dad46a0","1933e7d798f74194bcca25dc722d45d5","04a5aef3d83d475ea24f1040cffa89da","85ea790f52c7452b8f7f45629aa2e502","2b59f29ea77a4b16806e9e3b3e59541e","22f627ab23254ed19a6cd98f4d51a79e","6f35ec3785214171a315fccbfcd78826","e44364ebcd7f44a9a1c1e4792e1cc33d"]},"id":"CzKU-ktQPdCC","executionInfo":{"status":"ok","timestamp":1749649660507,"user_tz":-120,"elapsed":34593,"user":{"displayName":"Filippo CROCE","userId":"06292711882020938839"}},"outputId":"16fffa51-ebdc-4991-8b70-4496193dae87"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Could not load API key from Colab Secrets: name 'userdata' is not defined\n","Please use Option 2 below if you haven't set up the secret, OR ensure the secret name is 'GOOGLE_API_KEY'.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb5c86c1d9ff40b08b8cd4524d7f3385"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ROUGE metric evaluator loaded.\n"]}]},{"cell_type":"code","source":["GEMINI_MODEL_NAME = \"gemini-1.5-flash-latest\"\n","INPUT_PATH = \"/content/drive/MyDrive/Prompto_Ergo_Sum_HW2/dataset/eng/the_vampyre_subset.json\"\n","\n","OUTPUT_PATH_CLEANING_ONLY = \"/content/drive/MyDrive/Prompto_Ergo_Sum_HW2/results/cleaning_results.json\" # From cleaner_LLM.py\n","OUTPUT_PATH_JUDGING_ONLY = \"/content/drive/MyDrive/Prompto_Ergo_Sum_HW2/results/judging_results.json\" # From judge_LLM.py\n","OUTPUT_PATH_PIPELINE = \"/content/drive/MyDrive/Prompto_Ergo_Sum_HW2/results/full_pipeline_results.json\" # From main.py\n","\n","# Set to a small number for testing, or None to process all items in the dataset\n","NUM_ITEM_TO_PROCESS = 6 # Adjusted for the small dummy dataset\n","# NUM_ITEM_TO_PROCESS = None # To process all items\n","\n","print(\"Configuration set.\")\n","print(f\"GEMINI_MODEL_NAME: {GEMINI_MODEL_NAME}\")\n","print(f\"INPUT_PATH: {INPUT_PATH}\")\n","print(f\"OUTPUT_PATH_PIPELINE: {OUTPUT_PATH_PIPELINE}\")\n","print(f\"NUM_ITEM_TO_PROCESS: {NUM_ITEM_TO_PROCESS if NUM_ITEM_TO_PROCESS is not None else 'All'}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4jhcxzt-NMBl","executionInfo":{"status":"ok","timestamp":1749639105771,"user_tz":-120,"elapsed":5,"user":{"displayName":"Filippo CROCE","userId":"06292711882020938839"}},"outputId":"4cd9729a-838b-45f6-9374-48ef839b63f9"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Configuration set.\n","GEMINI_MODEL_NAME: gemini-1.5-flash-latest\n","INPUT_PATH: /content/drive/MyDrive/Prompto_Ergo_Sum_HW2/dataset/eng/the_vampyre_subset.json\n","OUTPUT_PATH_PIPELINE: /content/drive/MyDrive/Prompto_Ergo_Sum_HW2/results/full_pipeline_results.json\n","NUM_ITEM_TO_PROCESS: 6\n"]}]},{"cell_type":"code","source":["def clean_with_gemini(client: genai.Client, ocr_text: str) -> str:\n","    \"\"\"Cleans OCR text using Google Gemini (Client API style).\"\"\"\n","    if not ocr_text.strip():\n","        return \"\"\n","\n","    #try:\n","    #    # Initialize the client. Passing api_key explicitly is robust.\n","    #    client = genai.Client(api_key=GOOGLE_API_KEY)\n","    #except Exception as e:\n","    #    print(f\"Error initializing Gemini Client (genai.Client): {e}\")\n","    #    print(\"This could be due to an invalid API key, network issues, or problems with the 'google-generativeai' library.\")\n","    #    return f\"[GEMINI_CLIENT_INIT_ERROR: {e}]\"\n","\n","    contents = f\"\"\"Clean the following OCR text. Correct spelling errors, fix punctuation, remouve also the \\n present in the text. Preserve the original\n","    meaning and style. Do not add new information or summarize. Return only the cleaned text.\n","\n","OCR Text:\n","---\n","{ocr_text}\n","---\n","Cleaned Text:\n","\"\"\"\n","    response = client.models.generate_content(\n","        model=GEMINI_MODEL_NAME,\n","        contents=contents\n","    )\n","    #print(response.text)\n","    return response.text\n","\n","def clean_with_groq(client: Groq, ocr_text: str, model_id: str) -> str:\n","    \"\"\"Cleans OCR text using a model from the Groq API.\"\"\"\n","    if not ocr_text.strip():\n","        return \"\"\n","\n","    # This chat-based prompt works well for modern instruct models like Llama3 and Mixtral\n","    messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": \"Clean the following OCR text. Correct spelling errors, fix punctuation, remouve also the \\\\n present in the text. Preserve the original meaning and style. Do not add new information or summarize. Return only the cleaned text.\",\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": f\"OCR Text:\\n---\\n{ocr_text}\\n---\\nCleaned Text:\",\n","        }\n","    ]\n","\n","    try:\n","        chat_completion = client.chat.completions.create(\n","            messages=messages,\n","            model=model_id,\n","            temperature=0.1,  # Lower temperature for more deterministic output\n","            max_tokens=2048,\n","        )\n","        cleaned_text = chat_completion.choices[0].message.content.strip()\n","        return cleaned_text\n","    except Exception as e:\n","        return f\"[GROQ_API_ERROR: {e}]\"\n","\n","def judge_with_gemini(client: genai.Client, gemini_cleaned: str, ground_truth: str) -> str:\n","    \"\"\"Judges the quality of Gemini-generated text using a pre-initialized Gemini client.\"\"\"\n","\n","    # Handle empty input gracefully\n","    if not gemini_cleaned or not gemini_cleaned.strip():\n","        return \"0\" # Assign a score of 0 if the cleaned text is empty\n","\n","    #try:\n","    #    # Initialize the client. Passing api_key explicitly is robust.\n","    #    client = genai.Client(api_key=GOOGLE_API_KEY)\n","    #except Exception as e:\n","    #    print(f\"Error initializing Gemini Client (genai.Client): {e}\")\n","    #    print(\"This could be due to an invalid API key, network issues, or problems with the 'google-generativeai' library.\")\n","    #    return f\"[GEMINI_CLIENT_INIT_ERROR: {e}]\"\n","\n","    contents = f\"\"\"Evaluate the quality of the \"cleaned text\" against the \"ground truth\" reference.\n","Provide a score from 0 to 5 based on the following scale:\n","5: Perfect. The cleaned text fully and accurately matches the ground truth.\n","4: Excellent. Very minor errors (e.g., one or two typos, a single punctuation mistake) that do not affect meaning.\n","3: Good. Some errors persist (e.g., a few OCR mistakes) but the overall meaning is clear and correct.\n","2: Fair. Multiple issues make the text difficult to understand or it contains misleading information.\n","1: Poor. Unacceptable quality; the output is mostly unrelated, unreadable, or nonsensical.\n","0: Empty/No Output. The cleaned text was empty.\n","\n","---\n","[GROUND TRUTH]:\n","{ground_truth}\n","---\n","[CLEANED TEXT]:\n","{gemini_cleaned}\n","---\n","\n","Return ONLY the integer score (0-5) and nothing else.\n","\"\"\"\n","\n","    response = client.models.generate_content(\n","        model=GEMINI_MODEL_NAME,\n","        contents=contents\n","    )\n","    #print(response.text)\n","    return response.text\n"],"metadata":{"id":"jKfsG5H6ZCcT","executionInfo":{"status":"ok","timestamp":1749649711554,"user_tz":-120,"elapsed":28,"user":{"displayName":"Filippo CROCE","userId":"06292711882020938839"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def parse_score(response_text: str) -> int:\n","    \"\"\"Extracts the first integer from the judge's response for robustness.\"\"\"\n","    numbers = re.findall(r'\\d+', response_text)\n","    return int(numbers[0]) if numbers else -1 # -1 indicates a parsing error\n","\n","def calculate_metrics(reference: str, hypothesis: str) -> dict:\n","    \"\"\"Calculates WER and CER, handling edge cases.\"\"\"\n","    if not reference.strip() and not hypothesis.strip():\n","        return {\"wer\": 0.0, \"cer\": 0.0}\n","    if not reference.strip() or not hypothesis.strip():\n","        return {\"wer\": 1.0, \"cer\": 1.0}\n","\n","    return {\n","        \"wer\": jiwer.wer(reference, hypothesis),\n","        \"cer\": jiwer.cer(reference, hypothesis)\n","    }\n","\n","MODELS_TO_RUN = [\n","    {\n","        \"name\": \"Gemini-1.5-Flash\",\n","        \"type\": \"gemini\",\n","        \"function\": clean_with_gemini,\n","        \"model_id_or_client\": None  # Gemini uses a global client\n","    },\n","    {\n","        \"name\": \"Groq Llama-3-8B-Instruct\",\n","        \"type\": \"groq\",\n","        \"function\": clean_with_groq,\n","        \"model_id_or_client\": \"llama-3.3-70b-versatile\"\n","    }\n","]\n","\n","def get_detailed_diffs(text1: str, text2: str) -> list[dict]:\n","    \"\"\"\n","    Compares two strings word by word and returns a list of dictionaries\n","    highlighting the specific differing words.\n","    \"\"\"\n","    # Split texts into lists of words. We use a regex to better handle\n","    # punctuation, treating it as a separate \"word\".\n","    import re\n","    words1 = re.findall(r'\\w+|[^\\w\\s]', text1)\n","    words2 = re.findall(r'\\w+|[^\\w\\s]', text2)\n","\n","    s = difflib.SequenceMatcher(None, words1, words2)\n","    diffs = []\n","\n","    for tag, i1, i2, j1, j2 in s.get_opcodes():\n","        if tag == 'equal':\n","            continue\n","\n","        diff_item = {\n","            \"type\": tag,\n","            # Join the words to recreate the original text slice\n","            \"ground_truth_slice\": \" \".join(words1[i1:i2]),\n","            \"model_cleaned_slice\": \" \".join(words2[j1:j2])\n","        }\n","        diffs.append(diff_item)\n","\n","    return diffs\n","\n","print(\"Metrics and parsing functions defined.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ChLgGWvTDsC","executionInfo":{"status":"ok","timestamp":1749639105797,"user_tz":-120,"elapsed":14,"user":{"displayName":"Filippo CROCE","userId":"06292711882020938839"}},"outputId":"cb11eaa1-d4d6-47ff-d317-dc709bd4e539"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Metrics and parsing functions defined.\n"]}]},{"cell_type":"code","source":["def main():\n","    \"\"\"\n","    Main function to run the complete clean, evaluate, and judge pipeline.\n","    \"\"\"\n","    # --- Initialize APIs (ONCE) ---\n","    try:\n","        if not GOOGLE_API_KEY:\n","            raise ValueError(\"GOOGLE_API_KEY is not set. Please check the setup cell (Cell 3) and your Colab Secrets.\")\n","        gemini_client = genai.Client(api_key=GOOGLE_API_KEY)\n","\n","        if not GROQ_API_KEY:\n","            raise ValueError(\"GROQ_API_KEY is not set. Please check the setup cell (Cell 3) and your Colab Secrets.\")\n","        groq_client = Groq(api_key=GROQ_API_KEY)\n","\n","        rouge_metric = evaluate.load(\"rouge\")\n","        print(\"Successfully initialized Gemini Client and ROUGE metric evaluator.\")\n","    except Exception as e:\n","        print(f\"Fatal Error during initialization: {e}\")\n","        return\n","\n","    # --- Load Data ---\n","    try:\n","        with open(INPUT_PATH, 'r', encoding='utf-8') as f:\n","            data_dict = json.load(f)\n","        print(f\"Successfully loaded {len(data_dict)} items from '{INPUT_PATH}'\")\n","    except (FileNotFoundError, json.JSONDecodeError) as e:\n","        print(f\"Error loading input file: {e}\")\n","        return\n","\n","    list_of_items = list(data_dict.values())\n","    subset_to_process = list_of_items[:NUM_ITEM_TO_PROCESS] if NUM_ITEM_TO_PROCESS is not None else list_of_items\n","\n","    all_results = []\n","    print(f\"\\nStarting pipeline for {len(subset_to_process)} items across {len(MODELS_TO_RUN)} models...\")\n","\n","    # --- Process Each Item in the Dataset ---\n","    for i, item in enumerate(subset_to_process):\n","        print(f\"\\n{'='*20} Processing item {i+1}/{len(subset_to_process)} {'='*20}\")\n","\n","        ocr_text = item.get('ocr', '')\n","        ground_truth = item.get('clean', '')\n","\n","        # Structure to save all results for this item\n","        item_result = {\n","            \"item_id\": i + 1,\n","            \"original_ocr\": ocr_text,\n","            \"ground_truth\": ground_truth,\n","            \"model_outputs\": []\n","        }\n","\n","        # --- Loop through each configured model ---\n","        for model_config in MODELS_TO_RUN:\n","            model_name = model_config[\"name\"]\n","            cleaning_function = model_config[\"function\"]\n","            print(f\"\\n--- Running on model: {model_name} ---\")\n","\n","            # Step 1: Clean the text\n","            print(\"1. Cleaning text...\")\n","            cleaned_text = \"[ERROR: Unknown model type in config]\"\n","            if model_config[\"type\"] == \"gemini\":\n","                cleaned_text = cleaning_function(gemini_client, ocr_text)\n","            elif model_config[\"type\"] == \"huggingface\":\n","                model_id = model_config[\"model_id_or_client\"]\n","                cleaned_text = cleaning_function(model_id, ocr_text)\n","            elif model_config[\"type\"] == \"groq\":\n","                model_id = model_config[\"model_id_or_client\"]\n","                cleaned_text = cleaning_function(groq_client, ocr_text, model_id)\n","\n","            if \"[ERROR:\" in cleaned_text or \"[GEMINI_\" in cleaned_text or \"[HUGGINGFACE_\" in cleaned_text or \"[GROQ_\" in cleaned_text:\n","                print(f\"  -> Skipping further processing for {model_name} due to cleaning error: {cleaned_text}\")\n","                model_run_result = {\"model_name\": model_name, \"cleaned_text\": cleaned_text, \"metrics\": None, \"judgement\": None, \"differences\": []}\n","                item_result[\"model_outputs\"].append(model_run_result)\n","                time.sleep(1) # Still sleep to avoid hammering a failing API\n","                continue\n","\n","            # Step 2: Evaluate with quantitative metrics\n","            print(\"2. Calculating WER, CER, and ROUGE metrics...\")\n","            edit_metrics = calculate_metrics(ground_truth, cleaned_text)\n","            rouge_scores = rouge_metric.compute(predictions=[cleaned_text], references=[ground_truth])\n","\n","            # Step 3: Judge the quality with Gemini (using Gemini as the standard judge for all)\n","            print(\"3. Judging quality with Gemini...\")\n","            raw_judgement = judge_with_gemini(gemini_client, cleaned_text, ground_truth)\n","            parsed_judgement_score = parse_score(raw_judgement)\n","\n","            # Step 4: Get detailed differences\n","            detailed_differences = get_detailed_diffs(ground_truth, cleaned_text)\n","\n","            # Step 5: Aggregate all information for this model run\n","            model_run_result = {\n","                \"model_name\": model_name,\n","                \"cleaned_text\": cleaned_text,\n","                \"metrics\": {\n","                    \"wer\": edit_metrics['wer'],\n","                    \"cer\": edit_metrics['cer'],\n","                    \"rouge\": rouge_scores\n","                },\n","                \"judgement\": {\n","                    \"score\": parsed_judgement_score,\n","                    \"raw_score_text\": raw_judgement.strip()\n","                },\n","                \"differences\": detailed_differences\n","            }\n","            item_result[\"model_outputs\"].append(model_run_result)\n","\n","            print(f\"  -> Metrics: WER={edit_metrics['wer']:.4f}, CER={edit_metrics['cer']:.4f}\")\n","            print(f\"  -> Judgement: Score={parsed_judgement_score}\")\n","\n","            # Avoid hitting API rate limits\n","            time.sleep(1)\n","\n","        all_results.append(item_result)\n","\n","    # --- Save Final Combined Results ---\n","    print(\"\\n--- Pipeline Complete ---\")\n","    output_dir = os.path.dirname(OUTPUT_PATH_PIPELINE)\n","    if output_dir:\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","    try:\n","        with open(OUTPUT_PATH_PIPELINE, \"w\", encoding=\"utf-8\") as outfile:\n","            json.dump(all_results, outfile, indent=2, ensure_ascii=False)\n","        print(f\"\\nAll processed data and results saved to: {OUTPUT_PATH_PIPELINE}\")\n","    except IOError as e:\n","        print(f\"\\nError saving final results file: {e}\")\n","\n","    return all_results\n","\n","print(\"Main pipeline function defined.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"id":"gXSliSIETJYa","executionInfo":{"status":"error","timestamp":1749649411295,"user_tz":-120,"elapsed":32,"user":{"displayName":"Filippo CROCE","userId":"06292711882020938839"}},"outputId":"fa832028-44ab-4fc7-9541-7bdba3f7da16"},"execution_count":1,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"expected an indented block after 'if' statement on line 11 (<ipython-input-1-2361511132>, line 12)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-2361511132>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    rouge_metric = evaluate.load(\"rouge\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 11\n"]}]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    final_results = main()\n","\n","    if final_results:\n","        print(\"\\nScript finished successfully.\")\n","    else:\n","        print(\"\\nScript finished with an error.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EJrQZ67mj9iJ","executionInfo":{"status":"ok","timestamp":1749639125078,"user_tz":-120,"elapsed":19249,"user":{"displayName":"Filippo CROCE","userId":"06292711882020938839"}},"outputId":"be93f4f1-43fd-419c-eed6-3fd024073704"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully initialized Gemini Client and ROUGE metric evaluator.\n","Successfully loaded 24 items from '/content/drive/MyDrive/Prompto_Ergo_Sum_HW2/dataset/eng/the_vampyre_subset.json'\n","\n","Starting pipeline for 6 items...\n","\n","--- Processing item 1/6 ---\n","1. Cleaning text with Gemini...\n","2. Calculating WER, CER, and ROUGE metrics...\n","3. Judging quality with Gemini...\n","  -> Metrics: WER=0.0000, CER=0.0000\n","  -> Judgement: Score=5 (Raw: '5\n","')\n","\n","--- Processing item 2/6 ---\n","1. Cleaning text with Gemini...\n","2. Calculating WER, CER, and ROUGE metrics...\n","3. Judging quality with Gemini...\n","  -> Metrics: WER=0.0000, CER=0.0000\n","  -> Judgement: Score=5 (Raw: '5\n","')\n","\n","--- Processing item 3/6 ---\n","1. Cleaning text with Gemini...\n","2. Calculating WER, CER, and ROUGE metrics...\n","3. Judging quality with Gemini...\n","  -> Metrics: WER=0.0000, CER=0.0000\n","  -> Judgement: Score=5 (Raw: '5\n","')\n","\n","--- Processing item 4/6 ---\n","1. Cleaning text with Gemini...\n","2. Calculating WER, CER, and ROUGE metrics...\n","3. Judging quality with Gemini...\n","  -> Metrics: WER=0.0378, CER=0.0073\n","  -> Judgement: Score=4 (Raw: '4\n","')\n","\n","--- Processing item 5/6 ---\n","1. Cleaning text with Gemini...\n","2. Calculating WER, CER, and ROUGE metrics...\n","3. Judging quality with Gemini...\n","  -> Metrics: WER=0.0582, CER=0.0172\n","  -> Judgement: Score=4 (Raw: '4\n","')\n","\n","--- Processing item 6/6 ---\n","1. Cleaning text with Gemini...\n","2. Calculating WER, CER, and ROUGE metrics...\n","3. Judging quality with Gemini...\n","  -> Metrics: WER=0.1500, CER=0.0278\n","  -> Judgement: Score=4 (Raw: '4\n","')\n","\n","--- Pipeline Complete ---\n","\n","All processed data and results saved to: /content/drive/MyDrive/Prompto_Ergo_Sum_HW2/results/full_pipeline_results.json\n","\n","Script finished successfully.\n"]}]}]}